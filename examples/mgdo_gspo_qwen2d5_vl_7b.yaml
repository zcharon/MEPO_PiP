data:
  train_files: ./dataset/mmpr_tiny_mepo.jsonl
  val_files:   ./dataset/mmpr_tiny_mepo.jsonl
  prompt_key: question
  answer_key: answer
  ori_image_key: ori_image_lf
  one_image_key: one_image_lf
  system_prompt: A conversation between User and Assistant. Please answer the question based on the given image and return you thinking process step by step. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>.
  video_key: videos
  image_dir: null
  video_fps: 2.0
  max_prompt_length: 20000
  max_response_length: 8192
  rollout_batch_size: 192  # equivalent to verl's data.train_batch_size
  mini_rollout_batch_size: 8  # equivalent to verl's data.gen_batch_size (64)
  val_batch_size: 128
  override_chat_template: null
  shuffle: true
  seed: 1
  min_pixels: 50176
  max_pixels: 200704
  filter_overlong_prompts: true

algorithm:
  adv_estimator: mepo
  disable_kl: true
  use_kl_loss: true
  kl_penalty: low_var_kl
  kl_coef: 0.0
  online_filtering: True  # dapo filter groups
  filter_key: all_overall
  filter_low: 0.01
  filter_high: 0.99

worker:
  actor:
    global_batch_size: 192  # equivalent to verl's actor.ppo_mini_batch_size
    micro_batch_size_per_device_for_update: 1  # equivalent to verl's actor.ppo_micro_batch_size_per_gpu
    micro_batch_size_per_device_for_experience: 1  # equivalent to verl's rollout.log_prob_micro_batch_size_per_gpu
    max_grad_norm: 1.0
    padding_free: true
    dynamic_batching: true
    ulysses_size: 1
    clip_ratio_low: 3e-4
    clip_ratio_high: 4e-4
    loss_type: gspo_token
    loss_avg_mode: seq
    model:
      model_path: Qwen2.5-VL-7B-Instruct
      enable_gradient_checkpointing: true
      trust_remote_code: true
      freeze_vision_tower: false
    optim:
      lr: 1.0e-6
      weight_decay: 1.0e-2
      strategy: adamw  # {adamw, adamw_bf16}
      lr_warmup_ratio: 0.0
    fsdp:
      enable_full_shard: true
      enable_cpu_offload: false
      enable_rank0_init: true
    offload:
      offload_params: true  # true: more CPU memory; false: more GPU memory
      offload_optimizer: true  # true: more CPU memory; false: more GPU memory

  rollout:
    n: 8
    temperature: 1.0
    top_p: 1.0
    limit_images: 16
    max_num_batched_tokens: 29000
    gpu_memory_utilization: 0.7
    enforce_eager: false
    enable_chunked_prefill: false
    tensor_parallel_size: 4
    disable_tqdm: false
    val_override_config:
      temperature: 0.6
      top_p: 0.95
      n: 1

  ref:
    fsdp:
      enable_full_shard: true
      enable_cpu_offload: true  # true: more CPU memory; false: more GPU memory
      enable_rank0_init: true
    offload:
      offload_params: false

  reward:
    reward_type: sequential
    reward_function: ./examples/reward_function/r1v.py:compute_score

trainer:
  total_epochs: 30
  max_steps: null
  project_name: mepo
  experiment_name: qwen2d5_vl_7b_mepo
  logger: ["console", "wandb"]
  nnodes: 1
  n_gpus_per_node: 8
  max_try_make_batch: -1  # -1 means no limit
  val_freq: -1  # -1 to disable
  val_before_train: false
  val_only: false
  val_generations_to_log: 1
  save_freq: 50  # -1 to disable
  save_limit: -1  # -1 to disable
  save_model_only: false
  save_checkpoint_path: checkpoints/mepo/qwen2d5_vl_7b_mepo
  load_checkpoint_path: null
  find_last_checkpoint: true